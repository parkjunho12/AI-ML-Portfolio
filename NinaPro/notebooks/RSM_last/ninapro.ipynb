{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b01d87a-7a83-4a09-86a5-a7d51c17a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Unique labels: [ 0  1  3  4  6  9 10 11]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 393\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtcn_ninapro_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 345\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m train_data, val_data, train_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m    341\u001b[0m     train_data, train_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mtrain_labels\n\u001b[1;32m    342\u001b[0m )\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNinaProDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWINDOW_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m NinaProDataset(val_data, val_labels, WINDOW_SIZE)\n\u001b[1;32m    347\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m NinaProDataset(test_data, test_labels, WINDOW_SIZE)\n",
      "Cell \u001b[0;32mIn[13], line 116\u001b[0m, in \u001b[0;36mNinaProDataset.__init__\u001b[0;34m(self, data, labels, window_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m=\u001b[39m window_size\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Create sliding windows\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 132\u001b[0m, in \u001b[0;36mNinaProDataset._create_windows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m         windows\u001b[38;5;241m.\u001b[39mappend(window)\n\u001b[1;32m    130\u001b[0m         window_labels\u001b[38;5;241m.\u001b[39mappend(label \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert to 0-based indexing\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindows\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(window_labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        \n",
    "        for i in range(len(self.data) - self.window_size + 1):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size // 2]\n",
    "            \n",
    "            # Skip rest periods (label 0 if using 0-based indexing)\n",
    "            if label > 0:  # Adjust based on your labeling scheme\n",
    "                windows.append(window)\n",
    "                window_labels.append(label - 1)  # Convert to 0-based indexing\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.window_labels[idx]])[0]\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Unique labels: {np.unique(labels)}\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    # Split data\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        emg_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE)\n",
    "    val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE)\n",
    "    test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    num_classes = len(np.unique(labels)) - 1  # Excluding rest class\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3387da1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (200, 16)\n",
      "Window shape example: (200, 16)\n",
      "Window shape example: (200, 16)\n",
      "Training samples: 3434\n",
      "Validation samples: 862\n",
      "Test samples: 1056\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 428\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, label_mapping\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 401\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 401\u001b[0m train_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 244\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 244\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m    246\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 97\u001b[0m, in \u001b[0;36mTCN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Pass through TCN\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_channels[-1])\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 50\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 50\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out \u001b[38;5;241m+\u001b[39m res)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size // 2]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                windows.append(window)\n",
    "                # Map label to continuous 0-based indexing\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.window_labels[idx]])[0]\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            print(f\"Batch Data shape: {batch_data.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        emg_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    train_data = train_data[:len(train_data) // 2]\n",
    "    train_labels = train_labels[:len(train_labels) // 2] \n",
    "    val_data = val_data[:len(val_data) // 2]\n",
    "    val_labels = val_labels[:len(val_labels) // 2]  \n",
    "    test_data = test_data[:len(test_data) // 2]\n",
    "    test_labels = test_labels[:len(test_labels) // 2]  \n",
    "    \n",
    "    # Create datasets with label mapping\n",
    "    train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE, label_mapping)\n",
    "    val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE, label_mapping)\n",
    "    test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE, label_mapping)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "     \n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d808522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (16,)\n",
      "Window shape example: (16,)\n",
      "Window shape example: (16,)\n",
      "Training loader shape: 2\n",
      "Training samples: 3427\n",
      "Validation samples: 857\n",
      "Test samples: 1057\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 441\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, label_mapping\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 414\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 414\u001b[0m train_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 256\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    253\u001b[0m batch_data, batch_labels \u001b[38;5;241m=\u001b[39m batch_data\u001b[38;5;241m.\u001b[39mto(device), batch_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    255\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 256\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m    258\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 99\u001b[0m, in \u001b[0;36mTCN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Pass through TCN\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n\u001b[1;32m    102\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_channels[-1])\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 52\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 52\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out \u001b[38;5;241m+\u001b[39m res)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.unsqueeze(2) \n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size // 2]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                rms_features = np.sqrt(np.mean(window ** 2, axis=0))  # shape: (n_features,)\n",
    "                windows.append(rms_features)\n",
    "\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.window_labels[idx]])[0]\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "def bandpass_filter(signal, lowcut=20, highcut=450, fs=2000, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal, axis=0)\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "\n",
    "    #  remove noise\n",
    "    emg_data = bandpass_filter(emg_data, lowcut=20, highcut=450, fs=2000)\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        emg_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    train_data = train_data[:len(train_data) // 2]\n",
    "    train_labels = train_labels[:len(train_labels) // 2] \n",
    "    val_data = val_data[:len(val_data) // 2]\n",
    "    val_labels = val_labels[:len(val_labels) // 2]  \n",
    "    test_data = test_data[:len(test_data) // 2]\n",
    "    test_labels = test_labels[:len(test_labels) // 2]  \n",
    "    \n",
    "    # Create datasets with label mapping\n",
    "    train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE, label_mapping)\n",
    "    val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE, label_mapping)\n",
    "    test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE, label_mapping)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "     \n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34643fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (16,)\n",
      "Training loader shape: 2\n",
      "Training samples: 6816\n",
      "Validation samples: 1704\n",
      "Test samples: 2130\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n",
      "Epoch [10/100], Train Loss: 0.9545, Val Accuracy: 65.02%\n",
      "Epoch [20/100], Train Loss: 0.8232, Val Accuracy: 67.90%\n",
      "Epoch [30/100], Train Loss: 0.7492, Val Accuracy: 69.66%\n",
      "Epoch [40/100], Train Loss: 0.7012, Val Accuracy: 70.31%\n",
      "Epoch [50/100], Train Loss: 0.6629, Val Accuracy: 71.77%\n",
      "Epoch [60/100], Train Loss: 0.6102, Val Accuracy: 70.72%\n",
      "Epoch [70/100], Train Loss: 0.5785, Val Accuracy: 72.71%\n",
      "Epoch [80/100], Train Loss: 0.5416, Val Accuracy: 72.59%\n",
      "Epoch [90/100], Train Loss: 0.4478, Val Accuracy: 73.24%\n",
      "Epoch [100/100], Train Loss: 0.4304, Val Accuracy: 73.65%\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Final Test Accuracy: 0.7441\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       305\n",
      "           1       0.63      0.67      0.65       304\n",
      "           2       0.75      0.74      0.75       307\n",
      "           3       0.66      0.63      0.65       298\n",
      "           4       0.73      0.76      0.74       311\n",
      "           5       0.75      0.79      0.77       290\n",
      "           6       0.80      0.80      0.80       315\n",
      "\n",
      "    accuracy                           0.74      2130\n",
      "   macro avg       0.75      0.74      0.74      2130\n",
      "weighted avg       0.75      0.74      0.74      2130\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as 'tcn_ninapro_model.pth'\n"
     ]
    }
   ],
   "source": [
    "#   NinaDataset  train, test, val dataset split\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.unsqueeze(2) \n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size - 1]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                rms_features = np.sqrt(np.mean(window ** 2, axis=0))  # shape: (n_features,)\n",
    "                windows.append(rms_features)\n",
    "\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.windows[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.window_labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "def bandpass_filter(signal, lowcut=20, highcut=450, fs=2000, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal, axis=0)\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "\n",
    "    #  remove noise\n",
    "    emg_data = bandpass_filter(emg_data, lowcut=20, highcut=450, fs=2000)\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class EMGWindowDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    dataset = NinaProDataset(emg_data, labels, WINDOW_SIZE, label_mapping)\n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        dataset.windows, dataset.window_labels, test_size=0.2, random_state=42, stratify=dataset.window_labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    # train_data = train_data[:len(train_data) // 2]\n",
    "    # train_labels = train_labels[:len(train_labels) // 2] \n",
    "    # val_data = val_data[:len(val_data) // 2]\n",
    "    # val_labels = val_labels[:len(val_labels) // 2]  \n",
    "    # test_data = test_data[:len(test_data) // 2]\n",
    "    # test_labels = test_labels[:len(test_labels) // 2]  \n",
    "    \n",
    "    # # Create datasets with label mapping\n",
    "    # train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE, label_mapping)\n",
    "    # val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE, label_mapping)\n",
    "    # test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE, label_mapping)\n",
    "\n",
    "    train_dataset = EMGWindowDataset(train_data, train_labels)\n",
    "    val_dataset = EMGWindowDataset(val_data, val_labels)\n",
    "    test_dataset = EMGWindowDataset(test_data, test_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    batch = next(iter(train_loader))\n",
    "    print(f\"Training loader shape: {len(batch)}\")\n",
    "\n",
    "     \n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1df0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
