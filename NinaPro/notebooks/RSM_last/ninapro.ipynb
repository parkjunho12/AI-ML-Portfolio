{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b01d87a-7a83-4a09-86a5-a7d51c17a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Unique labels: [ 0  1  3  4  6  9 10 11]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 393\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtcn_ninapro_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 345\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m train_data, val_data, train_labels, val_labels \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m    341\u001b[0m     train_data, train_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, stratify\u001b[38;5;241m=\u001b[39mtrain_labels\n\u001b[1;32m    342\u001b[0m )\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# Create datasets\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNinaProDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWINDOW_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m NinaProDataset(val_data, val_labels, WINDOW_SIZE)\n\u001b[1;32m    347\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m NinaProDataset(test_data, test_labels, WINDOW_SIZE)\n",
      "Cell \u001b[0;32mIn[13], line 116\u001b[0m, in \u001b[0;36mNinaProDataset.__init__\u001b[0;34m(self, data, labels, window_size)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m=\u001b[39m window_size\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Create sliding windows\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 132\u001b[0m, in \u001b[0;36mNinaProDataset._create_windows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m         windows\u001b[38;5;241m.\u001b[39mappend(window)\n\u001b[1;32m    130\u001b[0m         window_labels\u001b[38;5;241m.\u001b[39mappend(label \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert to 0-based indexing\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindows\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(window_labels)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        \n",
    "        for i in range(len(self.data) - self.window_size + 1):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size // 2]\n",
    "            \n",
    "            # Skip rest periods (label 0 if using 0-based indexing)\n",
    "            if label > 0:  # Adjust based on your labeling scheme\n",
    "                windows.append(window)\n",
    "                window_labels.append(label - 1)  # Convert to 0-based indexing\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.window_labels[idx]])[0]\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Unique labels: {np.unique(labels)}\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    # Split data\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        emg_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE)\n",
    "    val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE)\n",
    "    test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    num_classes = len(np.unique(labels)) - 1  # Excluding rest class\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3387da1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (200, 16)\n",
      "Window shape example: (200, 16)\n",
      "Window shape example: (200, 16)\n",
      "Training samples: 3434\n",
      "Validation samples: 862\n",
      "Test samples: 1056\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n",
      "Batch Data shape: torch.Size([32, 200, 16])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 428\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, label_mapping\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 428\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 401\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 401\u001b[0m train_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[39], line 244\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch Data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 244\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m    246\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 97\u001b[0m, in \u001b[0;36mTCN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Pass through TCN\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_channels[-1])\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 50\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 50\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out \u001b[38;5;241m+\u001b[39m res)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size // 2]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                windows.append(window)\n",
    "                # Map label to continuous 0-based indexing\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.window_labels[idx]])[0]\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            print(f\"Batch Data shape: {batch_data.shape}\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        emg_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    train_data = train_data[:len(train_data) // 2]\n",
    "    train_labels = train_labels[:len(train_labels) // 2] \n",
    "    val_data = val_data[:len(val_data) // 2]\n",
    "    val_labels = val_labels[:len(val_labels) // 2]  \n",
    "    test_data = test_data[:len(test_data) // 2]\n",
    "    test_labels = test_labels[:len(test_labels) // 2]  \n",
    "    \n",
    "    # Create datasets with label mapping\n",
    "    train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE, label_mapping)\n",
    "    val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE, label_mapping)\n",
    "    test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE, label_mapping)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "     \n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d808522b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (16,)\n",
      "Window shape example: (16,)\n",
      "Window shape example: (16,)\n",
      "Training loader shape: 2\n",
      "Training samples: 3427\n",
      "Validation samples: 857\n",
      "Test samples: 1057\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 441\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, label_mapping\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 414\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 414\u001b[0m train_losses, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 256\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    253\u001b[0m batch_data, batch_labels \u001b[38;5;241m=\u001b[39m batch_data\u001b[38;5;241m.\u001b[39mto(device), batch_labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    255\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 256\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_labels)\n\u001b[1;32m    258\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 99\u001b[0m, in \u001b[0;36mTCN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Pass through TCN\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Global average pooling\u001b[39;00m\n\u001b[1;32m    102\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(y, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, num_channels[-1])\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 52\u001b[0m, in \u001b[0;36mTemporalBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 52\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out \u001b[38;5;241m+\u001b[39m res)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.unsqueeze(2) \n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size // 2]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                rms_features = np.sqrt(np.mean(window ** 2, axis=0))  # shape: (n_features,)\n",
    "                windows.append(rms_features)\n",
    "\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.windows[idx]), torch.LongTensor([self.window_labels[idx]])[0]\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "def bandpass_filter(signal, lowcut=20, highcut=450, fs=2000, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal, axis=0)\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "\n",
    "    #  remove noise\n",
    "    emg_data = bandpass_filter(emg_data, lowcut=20, highcut=450, fs=2000)\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        # scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        emg_data, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    train_data = train_data[:len(train_data) // 2]\n",
    "    train_labels = train_labels[:len(train_labels) // 2] \n",
    "    val_data = val_data[:len(val_data) // 2]\n",
    "    val_labels = val_labels[:len(val_labels) // 2]  \n",
    "    test_data = test_data[:len(test_data) // 2]\n",
    "    test_labels = test_labels[:len(test_labels) // 2]  \n",
    "    \n",
    "    # Create datasets with label mapping\n",
    "    train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE, label_mapping)\n",
    "    val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE, label_mapping)\n",
    "    test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE, label_mapping)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "     \n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "    \n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34643fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n",
      "Data shape: (1428729, 16)\n",
      "Labels shape: (1428729,)\n",
      "Original unique labels: [ 0  1  3  4  6  9 10 11]\n",
      "Gesture labels (excluding rest): [ 1  3  4  6  9 10 11]\n",
      "Number of gesture classes: 7\n",
      "Label mapping: {1: 0, 3: 1, 4: 2, 6: 3, 9: 4, 10: 5, 11: 6}\n",
      "Window shape example: (16,)\n",
      "Training loader shape: 2\n",
      "Training samples: 6816\n",
      "Validation samples: 1704\n",
      "Test samples: 2130\n",
      "Num features: 16\n",
      "Model initialized with 16 input features and 7 classes\n",
      "Total parameters: 223815\n",
      "\n",
      "Starting training...\n",
      "Epoch [10/100], Train Loss: 0.9792, Val Accuracy: 65.02%\n",
      "Epoch [20/100], Train Loss: 0.8446, Val Accuracy: 67.19%\n",
      "Epoch [30/100], Train Loss: 0.7646, Val Accuracy: 69.48%\n",
      "Epoch [40/100], Train Loss: 0.7154, Val Accuracy: 70.13%\n",
      "Epoch [50/100], Train Loss: 0.6080, Val Accuracy: 72.42%\n",
      "Epoch [60/100], Train Loss: 0.5975, Val Accuracy: 71.89%\n",
      "Epoch [70/100], Train Loss: 0.5815, Val Accuracy: 72.18%\n",
      "Epoch [80/100], Train Loss: 0.5718, Val Accuracy: 72.65%\n",
      "Epoch [90/100], Train Loss: 0.5658, Val Accuracy: 72.42%\n",
      "Epoch [100/100], Train Loss: 0.5490, Val Accuracy: 72.42%\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Final Test Accuracy: 0.7211\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.82      0.83       305\n",
      "           1       0.62      0.62      0.62       304\n",
      "           2       0.74      0.71      0.73       307\n",
      "           3       0.60      0.61      0.61       298\n",
      "           4       0.71      0.75      0.72       311\n",
      "           5       0.75      0.74      0.75       290\n",
      "           6       0.78      0.80      0.79       315\n",
      "\n",
      "    accuracy                           0.72      2130\n",
      "   macro avg       0.72      0.72      0.72      2130\n",
      "weighted avg       0.72      0.72      0.72      2130\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved as 'tcn_ninapro_model.pth'\n"
     ]
    }
   ],
   "source": [
    "#   NinaDataset  train, test, val dataset split\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Block for TCN implementation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs: int, n_outputs: int, kernel_size: int, stride: int, \n",
    "                 dilation: int, padding: int, dropout: float = 0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                                   stride=stride, padding=padding, \n",
    "                                                   dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        \n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Remove extra padding from the right side\n",
    "    \"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    Temporal Convolutional Network for NinaPro dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs: int, num_channels: List[int], num_classes: int, \n",
    "                 kernel_size: int = 2, dropout: float = 0.2):\n",
    "        super(TCN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            \n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, \n",
    "                                   stride=1, dilation=dilation_size,\n",
    "                                   padding=(kernel_size-1) * dilation_size, \n",
    "                                   dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, num_features)\n",
    "        # TCN expects: (batch_size, num_features, seq_len)\n",
    "        x = x.unsqueeze(2) \n",
    "        \n",
    "        # Pass through TCN\n",
    "        y = self.network(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        y = torch.mean(y, dim=2)  # (batch_size, num_channels[-1])\n",
    "        \n",
    "        # Classification\n",
    "        return self.classifier(y)\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size - 1]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                rms_features = np.sqrt(np.mean(window ** 2, axis=0))  # shape: (n_features,)\n",
    "                windows.append(rms_features)\n",
    "\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.windows[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.window_labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "def bandpass_filter(signal, lowcut=20, highcut=450, fs=2000, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal, axis=0)\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "\n",
    "    #  remove noise\n",
    "    emg_data = bandpass_filter(emg_data, lowcut=20, highcut=450, fs=2000)\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class EMGWindowDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    dataset = NinaProDataset(emg_data, labels, WINDOW_SIZE, label_mapping)\n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        dataset.windows, dataset.window_labels, test_size=0.2, random_state=42, stratify=dataset.window_labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    # train_data = train_data[:len(train_data) // 2]\n",
    "    # train_labels = train_labels[:len(train_labels) // 2] \n",
    "    # val_data = val_data[:len(val_data) // 2]\n",
    "    # val_labels = val_labels[:len(val_labels) // 2]  \n",
    "    # test_data = test_data[:len(test_data) // 2]\n",
    "    # test_labels = test_labels[:len(test_labels) // 2]  \n",
    "    \n",
    "    # # Create datasets with label mapping\n",
    "    # train_dataset = NinaProDataset(train_data, train_labels, WINDOW_SIZE, label_mapping)\n",
    "    # val_dataset = NinaProDataset(val_data, val_labels, WINDOW_SIZE, label_mapping)\n",
    "    # test_dataset = NinaProDataset(test_data, test_labels, WINDOW_SIZE, label_mapping)\n",
    "\n",
    "    train_dataset = EMGWindowDataset(train_data, train_labels)\n",
    "    val_dataset = EMGWindowDataset(val_data, val_labels)\n",
    "    test_dataset = EMGWindowDataset(test_data, test_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    batch = next(iter(train_loader))\n",
    "    print(f\"Training loader shape: {len(batch)}\")\n",
    "\n",
    "     \n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "\n",
    "    print(f\"Num features: {num_features}\")\n",
    "    model = TCN(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1df0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
