{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch and snntorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "from snntorch import utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 데이터 경로 설정\n",
    "DATA_PATH = \"../../data/DB6/DB6_s1_a/S1_D1_T1.mat\"\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_ninapro_data(file_path):\n",
    "    \"\"\"\n",
    "    NinaPro 데이터를 로드하고 전처리하는 함수\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # .mat 파일 로드\n",
    "        data = sio.loadmat(file_path)\n",
    "        \n",
    "        # 일반적인 NinaPro DB6 구조에 따른 키 확인\n",
    "        print(\"Available keys in the data:\", list(data.keys()))\n",
    "        \n",
    "        # 일반적으로 사용되는 키들 (실제 데이터에 따라 조정 필요)\n",
    "        if 'emg' in data:\n",
    "            emg_data = data['emg']\n",
    "        elif 'data' in data:\n",
    "            emg_data = data['data']\n",
    "        else:\n",
    "            # 키를 찾아서 EMG 데이터 추출\n",
    "            data_keys = [k for k in data.keys() if not k.startswith('__')]\n",
    "            emg_data = data[data_keys[0]]\n",
    "        \n",
    "        if 'stimulus' in data:\n",
    "            labels = data['stimulus'].flatten()\n",
    "        elif 'restimulus' in data:\n",
    "            labels = data['restimulus'].flatten()\n",
    "        elif 'glove' in data:\n",
    "            labels = data['glove']\n",
    "            if labels.ndim > 1:\n",
    "                labels = labels[:, 0]  # 첫 번째 열 사용\n",
    "        else:\n",
    "            # 라벨 데이터 찾기\n",
    "            label_keys = [k for k in data.keys() if 'stimulus' in k.lower() or 'label' in k.lower()]\n",
    "            if label_keys:\n",
    "                labels = data[label_keys[0]].flatten()\n",
    "            else:\n",
    "                # 두 번째로 큰 배열을 라벨로 가정\n",
    "                data_keys = [k for k in data.keys() if not k.startswith('__')]\n",
    "                labels = data[data_keys[1]].flatten() if len(data_keys) > 1 else np.zeros(emg_data.shape[0])\n",
    "        \n",
    "        print(f\"EMG data shape: {emg_data.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")\n",
    "        print(f\"Unique labels: {np.unique(labels)}\")\n",
    "        \n",
    "        return emg_data, labels\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        # 샘플 데이터 생성 (실제 데이터가 없을 경우)\n",
    "        print(\"Generating sample data for demonstration...\")\n",
    "        n_samples = 10000\n",
    "        n_channels = 12\n",
    "        emg_data = np.random.randn(n_samples, n_channels) * 0.1\n",
    "        # EMG 신호처럼 보이도록 노이즈 추가\n",
    "        for i in range(n_channels):\n",
    "            emg_data[:, i] += np.sin(np.linspace(0, 100*np.pi, n_samples)) * 0.05\n",
    "        labels = np.random.randint(0, 7, n_samples)  # 0-6 클래스\n",
    "        return emg_data, labels\n",
    "\n",
    "def preprocess_data_for_snn(emg_data, labels, window_size=200, overlap=100):\n",
    "    \"\"\"\n",
    "    SNN을 위한 EMG 데이터 전처리\n",
    "    \"\"\"\n",
    "    # 레이블이 0인 rest 구간 제거 (선택사항)\n",
    "    non_zero_mask = labels != 0\n",
    "    emg_data = emg_data[non_zero_mask]\n",
    "    labels = labels[non_zero_mask]\n",
    "    \n",
    "    # 윈도우 기반 특징 추출\n",
    "    windowed_features = []\n",
    "    windowed_labels = []\n",
    "    \n",
    "    step_size = window_size - overlap\n",
    "    \n",
    "    for i in range(0, len(emg_data) - window_size + 1, step_size):\n",
    "        window = emg_data[i:i+window_size]\n",
    "        window_label = labels[i:i+window_size]\n",
    "        \n",
    "        # 윈도우 내에서 가장 빈번한 라벨 사용\n",
    "        unique_labels, counts = np.unique(window_label, return_counts=True)\n",
    "        dominant_label = unique_labels[np.argmax(counts)]\n",
    "        \n",
    "        # 특징 추출 (시간 도메인 및 주파수 도메인)\n",
    "        features = []\n",
    "        for channel in range(window.shape[1]):\n",
    "            channel_data = window[:, channel]\n",
    "            \n",
    "            # 시간 도메인 특징들\n",
    "            features.extend([\n",
    "                np.mean(channel_data),                    # 평균\n",
    "                np.std(channel_data),                     # 표준편차\n",
    "                np.var(channel_data),                     # 분산\n",
    "                np.max(channel_data),                     # 최대값\n",
    "                np.min(channel_data),                     # 최소값\n",
    "                np.median(channel_data),                  # 중앙값\n",
    "                np.mean(np.abs(channel_data)),           # 평균 절대값\n",
    "                np.sqrt(np.mean(channel_data**2)),       # RMS\n",
    "                np.percentile(channel_data, 25),         # 25분위수\n",
    "                np.percentile(channel_data, 75),         # 75분위수\n",
    "                np.mean(np.abs(np.diff(channel_data))),  # 평균 변화율\n",
    "                len(np.where(np.diff(np.sign(channel_data)))[0]) / len(channel_data)  # Zero crossing rate\n",
    "            ])\n",
    "        \n",
    "        windowed_features.append(features)\n",
    "        windowed_labels.append(dominant_label)\n",
    "    \n",
    "    return np.array(windowed_features), np.array(windowed_labels)\n",
    "\n",
    "class SNNNet(nn.Module):\n",
    "    \"\"\"\n",
    "    snntorch를 사용한 Spiking Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, num_steps, \n",
    "                 beta=0.95, threshold=1.0, spike_grad=surrogate.fast_sigmoid()):\n",
    "        super(SNNNet, self).__init__()\n",
    "        \n",
    "        self.num_steps = num_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # 네트워크 레이어 정의\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.lif_layers = nn.ModuleList()\n",
    "        \n",
    "        # 입력 레이어\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # 히든 레이어들\n",
    "        for hidden_size in hidden_sizes:\n",
    "            self.layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            self.lif_layers.append(snn.Leaky(beta=beta, threshold=threshold, \n",
    "                                           spike_grad=spike_grad, \n",
    "                                           reset_mechanism=\"subtract\",\n",
    "                                           learn_beta=True,\n",
    "                                           learn_threshold=True))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # 출력 레이어\n",
    "        self.layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.lif_output = snn.Leaky(beta=beta*0.8, threshold=threshold*0.9, \n",
    "                                  spike_grad=spike_grad,\n",
    "                                  reset_mechanism=\"subtract\",\n",
    "                                  learn_beta=True,\n",
    "                                  learn_threshold=True,\n",
    "                                  output=True)\n",
    "        \n",
    "        # 드롭아웃\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 히든 레이어 상태 초기화\n",
    "        mem_layers = []\n",
    "        for lif_layer in self.lif_layers:\n",
    "            mem_layers.append(lif_layer.init_leaky())\n",
    "        mem_output = self.lif_output.init_leaky()\n",
    "        \n",
    "        # 스파이크 기록\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "        \n",
    "        for step in range(self.num_steps):\n",
    "            # 입력 스파이크 생성 (Rate coding 또는 Latency coding)\n",
    "            if step == 0:\n",
    "                # 첫 번째 스텝에서만 입력 (정적 입력을 스파이크로 변환)\n",
    "                cur_input = spikegen.rate(x, num_steps=1).squeeze(0)\n",
    "            else:\n",
    "                # Rate coding: 확률적 스파이크 생성\n",
    "                cur_input = torch.bernoulli(torch.sigmoid(x))\n",
    "            \n",
    "            # 순전파\n",
    "            cur = cur_input\n",
    "            \n",
    "            # 히든 레이어들\n",
    "            for i, (layer, lif_layer) in enumerate(zip(self.layers[:-1], self.lif_layers)):\n",
    "                cur = layer(cur)\n",
    "                if i > 0:  # 첫 번째 레이어가 아닌 경우 드롭아웃 적용\n",
    "                    cur = self.dropout(cur)\n",
    "                spk, mem_layers[i] = lif_layer(cur, mem_layers[i])\n",
    "                cur = spk\n",
    "            \n",
    "            # 출력 레이어\n",
    "            cur = self.layers[-1](cur)\n",
    "            spk_out, mem_output = self.lif_output(cur, mem_output)\n",
    "            \n",
    "            spk_rec.append(spk_out)\n",
    "            mem_rec.append(mem_output)\n",
    "        \n",
    "        return torch.stack(spk_rec), torch.stack(mem_rec)\n",
    "\n",
    "def create_data_loaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=32):\n",
    "    \"\"\"\n",
    "    PyTorch DataLoader 생성\n",
    "    \"\"\"\n",
    "    # numpy to tensor 변환\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.LongTensor(y_val)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test)\n",
    "    \n",
    "    # Dataset 생성\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_snn(model, train_loader, val_loader, num_epochs=100, lr=0.001):\n",
    "    \"\"\"\n",
    "    SNN 모델 학습\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    \n",
    "    # 손실 함수 (스파이크 기반)\n",
    "    loss_fn = SF.ce_rate_loss()  # Classification error based on spike rate\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting SNN training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 학습 모드\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 순전파\n",
    "            spk_rec, mem_rec = model(data)\n",
    "            \n",
    "            # 손실 계산 (스파이크 개수 기반)\n",
    "            loss = loss_fn(spk_rec, targets)\n",
    "            \n",
    "            # 역전파\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # 정확도 계산 (총 스파이크 개수로 예측)\n",
    "            _, predicted = torch.max(spk_rec.sum(dim=0), 1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(device), targets.to(device)\n",
    "                \n",
    "                spk_rec, mem_rec = model(data)\n",
    "                loss = loss_fn(spk_rec, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(spk_rec.sum(dim=0), 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # 평균 계산\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        train_acc = 100. * train_correct / train_total\n",
    "        val_acc = 100. * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # 학습률 스케줄링\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            # 최고 모델 저장\n",
    "            torch.save(model.state_dict(), 'best_snn_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "                  f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, '\n",
    "                  f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1}')\n",
    "            break\n",
    "    \n",
    "    # 최고 모델 로드\n",
    "    model.load_state_dict(torch.load('best_snn_model.pth'))\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'train_accuracy': train_accuracies,\n",
    "        'val_loss': val_losses,\n",
    "        'val_accuracy': val_accuracies\n",
    "    }\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_snn(model, test_loader):\n",
    "    \"\"\"\n",
    "    SNN 모델 평가\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            spk_rec, mem_rec = model(data)\n",
    "            \n",
    "            # 예측 (총 스파이크 개수 기반)\n",
    "            _, predicted = torch.max(spk_rec.sum(dim=0), 1)\n",
    "            \n",
    "            test_total += targets.size(0)\n",
    "            test_correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    test_acc = 100. * test_correct / test_total\n",
    "    \n",
    "    return test_acc, np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "def plot_spike_analysis(model, sample_data, num_steps):\n",
    "    \"\"\"\n",
    "    스파이크 분석 및 시각화\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_input = sample_data[:1].to(device)  # 첫 번째 샘플\n",
    "        spk_rec, mem_rec = model(sample_input)\n",
    "        \n",
    "        # 스파이크 래스터 플롯\n",
    "        spk_rec_np = spk_rec.squeeze().cpu().numpy()  # (num_steps, output_size)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # 서브플롯 1: 스파이크 래스터\n",
    "        plt.subplot(2, 2, 1)\n",
    "        for neuron in range(spk_rec_np.shape[1]):\n",
    "            spike_times = np.where(spk_rec_np[:, neuron] > 0)[0]\n",
    "            plt.scatter(spike_times, [neuron] * len(spike_times), s=5, alpha=0.7)\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Output Neuron')\n",
    "        plt.title('Output Spike Raster Plot')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 서브플롯 2: 스파이크 빈도\n",
    "        plt.subplot(2, 2, 2)\n",
    "        spike_rates = np.mean(spk_rec_np, axis=0)\n",
    "        plt.bar(range(len(spike_rates)), spike_rates)\n",
    "        plt.xlabel('Output Neuron')\n",
    "        plt.ylabel('Spike Rate')\n",
    "        plt.title('Output Spike Rates')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 서브플롯 3: 막전위 변화\n",
    "        plt.subplot(2, 2, 3)\n",
    "        mem_rec_np = mem_rec.squeeze().cpu().numpy()\n",
    "        for i in range(min(3, mem_rec_np.shape[1])):  # 처음 3개 뉴런만\n",
    "            plt.plot(mem_rec_np[:, i], label=f'Neuron {i}', alpha=0.7)\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Membrane Potential')\n",
    "        plt.title('Membrane Potential Evolution')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 서브플롯 4: 누적 스파이크\n",
    "        plt.subplot(2, 2, 4)\n",
    "        cumulative_spikes = np.cumsum(spk_rec_np, axis=0)\n",
    "        for i in range(min(3, cumulative_spikes.shape[1])):\n",
    "            plt.plot(cumulative_spikes[:, i], label=f'Neuron {i}', alpha=0.7)\n",
    "        plt.xlabel('Time Steps')\n",
    "        plt.ylabel('Cumulative Spikes')\n",
    "        plt.title('Cumulative Spike Count')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    학습 과정 시각화\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = range(1, len(history['train_accuracy']) + 1)\n",
    "    \n",
    "    # 정확도 플롯\n",
    "    axes[0, 0].plot(epochs, history['train_accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 0].set_title('SNN Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 손실 플롯\n",
    "    axes[0, 1].plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 1].set_title('SNN Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 마지막 10 에포크 정확도\n",
    "    start_epoch = max(0, len(history['train_accuracy']) - 10)\n",
    "    recent_epochs = epochs[start_epoch:]\n",
    "    \n",
    "    axes[1, 0].plot(recent_epochs, history['train_accuracy'][start_epoch:], \n",
    "                   'b-', label='Training Accuracy', linewidth=2, marker='o')\n",
    "    axes[1, 0].plot(recent_epochs, history['val_accuracy'][start_epoch:], \n",
    "                   'r-', label='Validation Accuracy', linewidth=2, marker='s')\n",
    "    axes[1, 0].set_title('Last 10 Epochs - Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy (%)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # SNN 특성 정보\n",
    "    axes[1, 1].text(0.5, 0.5, \n",
    "                   'snntorch SNN Features\\n\\n• Leaky Integrate-and-Fire\\n• Learnable Parameters\\n• Surrogate Gradients\\n• Spike-based Computation\\n• Temporal Dynamics\\n• Energy Efficient', \n",
    "                   ha='center', va='center', transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\", alpha=0.5))\n",
    "    axes[1, 1].set_xlim(0, 1)\n",
    "    axes[1, 1].set_ylim(0, 1)\n",
    "    axes[1, 1].set_title('snntorch SNN Info', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.suptitle('snntorch SNN Training Results', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"\n",
    "    혼동 행렬 시각화\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('snntorch SNN - Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"=== NinaPro EMG Classification with snntorch ===\")\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    \n",
    "    # 2. 데이터 전처리\n",
    "    print(\"\\n2. Preprocessing data for SNN...\")\n",
    "    X, y = preprocess_data_for_snn(emg_data, labels, window_size=200, overlap=100)\n",
    "    \n",
    "    print(f\"Feature data shape: {X.shape}\")\n",
    "    print(f\"Labels shape: {y.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "    \n",
    "    # 3. 라벨 인코딩\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(np.unique(y_encoded))\n",
    "    class_names = [f\"Gesture {i}\" for i in range(num_classes)]\n",
    "    \n",
    "    # 4. 데이터 분할\n",
    "    print(\"\\n3. Splitting data...\")\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.15, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # 5. 특징 정규화\n",
    "    print(\"\\n4. Normalizing features...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # 6. DataLoader 생성\n",
    "    print(\"\\n5. Creating data loaders...\")\n",
    "    train_loader, val_loader, test_loader = create_data_loaders(\n",
    "        X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # 7. SNN 모델 생성\n",
    "    print(\"\\n6. Creating snntorch SNN model...\")\n",
    "    input_size = X_train_scaled.shape[1]\n",
    "    hidden_sizes = [256, 128, 64]\n",
    "    num_steps = 25\n",
    "    \n",
    "    model = SNNNet(\n",
    "        input_size=input_size,\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        output_size=num_classes,\n",
    "        num_steps=num_steps,\n",
    "        beta=0.95,  # 누설 계수 (높을수록 긴 메모리)\n",
    "        threshold=1.0,  # 스파이크 임계값\n",
    "        spike_grad=surrogate.fast_sigmoid(slope=25)  # 대리 기울기\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "    print(model)\n",
    "    \n",
    "    # 8. 모델 학습\n",
    "    print(\"\\n7. Training snntorch SNN model...\")\n",
    "    model, history = train_snn(model, train_loader, val_loader, \n",
    "                              num_epochs=100, lr=0.001)\n",
    "    \n",
    "    # 9. 최종 평가\n",
    "    print(\"\\n8. Evaluating SNN model...\")\n",
    "    test_acc, y_pred, y_true = evaluate_snn(model, test_loader)\n",
    "    print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    # 10. 결과 시각화\n",
    "    print(\"\\n9. Plotting results...\")\n",
    "    \n",
    "    # 학습 과정 시각화\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # 혼동 행렬 시각화\n",
    "    plot_confusion_matrix(y_true, y_pred, class_names)\n",
    "    \n",
    "    # 스파이크 분석\n",
    "    print(\"\\n10. Spike analysis...\")\n",
    "    sample_tensor = torch.FloatTensor(X_test_scaled[:5])\n",
    "    plot_spike_analysis(model, sample_tensor, num_steps)\n",
    "    \n",
    "    # 분류 리포트\n",
    "    print(\"\\n11. Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # 최종 결과 요약\n",
    "    print(\"\\n=== Final snntorch SNN Results Summary ===\")\n",
    "    print(f\"Training Accuracy: {max(history['train_accuracy']):.2f}%\")\n",
    "    print(f\"Validation Accuracy: {max(history['val_accuracy']):.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"Total Epochs Trained: {len(history['train_accuracy'])}\")\n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Time Steps: {num_steps}\")\n",
    "    print(f\"Hidden Layers: {hidden_sizes}\")\n",
    "    \n",
    "    return model, history, scaler, label_encoder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # snntorch 설치 확인\n",
    "    try:\n",
    "        import snntorch\n",
    "        print(f\"snntorch version: {snntorch.__version__}\")\n",
    "    except ImportError:\n",
    "        print(\"Please install snntorch: pip install snntorch\")\n",
    "        exit(1)\n",
    "    \n",
    "    model, history, scaler, label_encoder = main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
