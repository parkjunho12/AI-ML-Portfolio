{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 먼저 NinaDataset전환 후 train, test, val dataset으로 split\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from typing import Tuple, List\n",
    "from scipy.signal import butter, filtfilt\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "from utils.utils import MLPClassifier\n",
    "\n",
    "\n",
    "class NinaProDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for NinaPro data\n",
    "    \"\"\"\n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, window_size: int = 200, label_mapping: dict = None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.window_size = window_size\n",
    "        self.label_mapping = label_mapping\n",
    "        \n",
    "        # Create sliding windows\n",
    "        self.windows, self.window_labels = self._create_windows()\n",
    "        print(\"Window shape example:\", self.windows[0].shape)\n",
    "        \n",
    "    def _create_windows(self):\n",
    "        windows = []\n",
    "        window_labels = []\n",
    "        stride = self.window_size // 2\n",
    "\n",
    "        for i in range(0, len(self.data) - self.window_size + 1, stride):\n",
    "            window = self.data[i:i + self.window_size]\n",
    "            # Use the label of the center point of the window\n",
    "            label = self.labels[i + self.window_size - 1]\n",
    "            \n",
    "            # Skip rest periods (label 0 in NinaPro)\n",
    "            if label > 0:\n",
    "                windows.append(window)\n",
    "\n",
    "                mapped_label = self.label_mapping[label] if self.label_mapping else label - 1\n",
    "                window_labels.append(mapped_label)\n",
    "                \n",
    "        return np.array(windows), np.array(window_labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.windows[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.window_labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def load_ninapro_data(file_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load NinaPro .mat file\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the .mat file\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (EMG data, labels)\n",
    "    \"\"\"\n",
    "    mat_data = sio.loadmat(file_path)\n",
    "    \n",
    "    # Common key names in NinaPro dataset\n",
    "    # Adjust these based on your specific dataset structure\n",
    "    possible_emg_keys = ['emg', 'EMG', 'data']\n",
    "    possible_label_keys = ['restimulus', 'stimulus', 'labels', 'rerepetition']\n",
    "    \n",
    "    emg_data = None\n",
    "    labels = None\n",
    "    \n",
    "    # Find EMG data\n",
    "    for key in possible_emg_keys:\n",
    "        if key in mat_data:\n",
    "            emg_data = mat_data[key]\n",
    "            break\n",
    "    \n",
    "    # Find labels\n",
    "    for key in possible_label_keys:\n",
    "        if key in mat_data:\n",
    "            labels = mat_data[key].flatten()\n",
    "            break\n",
    "    \n",
    "    if emg_data is None or labels is None:\n",
    "        print(\"Available keys in .mat file:\")\n",
    "        for key in mat_data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                print(f\"  {key}: {mat_data[key].shape if hasattr(mat_data[key], 'shape') else type(mat_data[key])}\")\n",
    "        raise ValueError(\"Could not find EMG data or labels. Please check the keys above.\")\n",
    "    \n",
    "    return emg_data, labels\n",
    "\n",
    "def bandpass_filter(signal, lowcut=20, highcut=450, fs=2000, order=4):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return filtfilt(b, a, signal, axis=0)\n",
    "\n",
    "def preprocess_data(emg_data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray, dict, int]:\n",
    "    \"\"\"\n",
    "    Preprocess EMG data and labels\n",
    "    \"\"\"\n",
    "    # Remove any NaN values\n",
    "    valid_indices = ~np.isnan(emg_data).any(axis=1) & ~np.isnan(labels)\n",
    "    emg_data = emg_data[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "\n",
    "    # ✨ remove noise\n",
    "    emg_data = bandpass_filter(emg_data, lowcut=20, highcut=450, fs=2000)\n",
    "    \n",
    "    # Normalize EMG data\n",
    "    scaler = StandardScaler()\n",
    "    emg_data = scaler.fit_transform(emg_data)\n",
    "    \n",
    "    # Create label mapping for continuous indexing\n",
    "    unique_labels = np.unique(labels)\n",
    "    # Remove rest label (0) if present\n",
    "    gesture_labels = unique_labels[unique_labels > 0]\n",
    "    \n",
    "    # Create mapping from original labels to 0-based continuous labels\n",
    "    label_mapping = {label: idx for idx, label in enumerate(gesture_labels)}\n",
    "    num_classes = len(gesture_labels)\n",
    "    \n",
    "    print(f\"Data shape: {emg_data.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Original unique labels: {unique_labels}\")\n",
    "    print(f\"Gesture labels (excluding rest): {gesture_labels}\")\n",
    "    print(f\"Number of gesture classes: {num_classes}\")\n",
    "    print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    return emg_data, labels, label_mapping, num_classes\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, \n",
    "                num_epochs: int = 100, lr: float = 0.001, num_steps: int = 10) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Train the TCN model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, '\n",
    "                  f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader, num_steps: int = 10) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data, batch_labels in test_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    report = classification_report(all_labels, all_predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "def plot_training_history(train_losses: List[float], val_accuracies: List[float]):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    \n",
    "    ax2.plot(val_accuracies)\n",
    "    ax2.set_title('Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "class EMGWindowDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.y[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main training pipeline\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    DATA_PATH = \"../../data/DB6/DB6_s1_a/S1_D1_T1.mat\"  # Update this path\n",
    "    TEST_DATA_PATH = \"../data/DB6/DB6_s1_a/S1_D1_T2.mat\" \n",
    "    WINDOW_SIZE = 200  # Adjust based on your sampling rate and desired window length\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 100\n",
    "    LEARNING_RATE = 0.001\n",
    "    \n",
    "    # TCN hyperparameters\n",
    "    TCN_CHANNELS = [64, 64, 128, 128] \n",
    "    #[64, 64, 128, 128]  # Hidden layer sizes\n",
    "    KERNEL_SIZE = 3\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load data\n",
    "    emg_data, labels = load_ninapro_data(DATA_PATH)\n",
    "    emg_data, labels, label_mapping, num_classes = preprocess_data(emg_data, labels)\n",
    "    \n",
    "    dataset = NinaProDataset(emg_data, labels, WINDOW_SIZE, label_mapping)\n",
    "    # Split data\n",
    "\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "        dataset.windows, dataset.window_labels, test_size=0.2, random_state=42, stratify=dataset.window_labels\n",
    "    )\n",
    "    \n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_data, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n",
    "    )\n",
    "    \n",
    "    train_dataset = EMGWindowDataset(train_data, train_labels)\n",
    "    val_dataset = EMGWindowDataset(val_data, val_labels)\n",
    "    test_dataset = EMGWindowDataset(test_data, test_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    batch = next(iter(train_loader))\n",
    "    print(f\"Training loader shape: {len(batch)}\")\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    num_features = emg_data.shape[1]\n",
    "\n",
    "    print(f\"Num features: {num_features}\")\n",
    "    model = TCNGen(num_inputs=num_features, \n",
    "                num_channels=TCN_CHANNELS,\n",
    "                num_classes=num_classes,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                dropout=DROPOUT,\n",
    "                timestamp=10)\n",
    "    \n",
    "    print(f\"Model initialized with {num_features} input features and {num_classes} classes\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nStarting training...\")\n",
    "    train_losses, val_accuracies = train_model(model, train_loader, val_loader, \n",
    "                                             NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_accuracy, classification_rep = evaluate_model(model, test_loader)\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_rep)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_accuracies)\n",
    "    \n",
    "    # Save model and label mapping\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'label_mapping': label_mapping,\n",
    "        'num_classes': num_classes,\n",
    "        'num_features': num_features\n",
    "    }, 'tcn_ninapro_model.pth')\n",
    "    print(\"\\nModel saved as 'tcn_ninapro_model.pth'\")\n",
    "    \n",
    "    return model, label_mapping"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
